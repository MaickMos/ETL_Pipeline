"""
DAG principal para el pipeline ETL
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
import sys
import os

# AÃ±adir el directorio raÃ­z al path para imports
sys.path.insert(0, '/opt/airflow')

# Importar funciones ETL
from etl.extract import extract_data
from etl.transform import transform_data
from etl.load import load_data

# ConfiguraciÃ³n por defecto del DAG
default_args = {
    'owner': 'data-engineer',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Definir el DAG
dag = DAG(
    'etl_pipeline',
    default_args=default_args,
    description='Pipeline ETL completo',
    schedule_interval=timedelta(days=1),  # Ejecutar diariamente
    catchup=False,
    tags=['etl', 'data-pipeline'],
)

def extract_task(**context):
    """
    Task para extraer datos
    """
    print("ğŸ” Iniciando extracciÃ³n de datos...")
    
    # Llamar a tu funciÃ³n de extracciÃ³n
    data = extract_data()
    
    print(f"âœ… ExtracciÃ³n completada. Registros extraÃ­dos: {len(data) if data is not None else 0}")
    
    # Pasar datos al siguiente task usando XCom
    return {"status": "success", "records": len(data) if data is not None else 0}

def transform_task(**context):
    """
    Task para transformar datos
    """
    print("ğŸ”„ Iniciando transformaciÃ³n de datos...")
    
    # Obtener resultado del task anterior
    extract_result = context['task_instance'].xcom_pull(task_ids='extract_data')
    print(f"ğŸ“Š Resultado de extracciÃ³n: {extract_result}")
    
    # Llamar a tu funciÃ³n de transformaciÃ³n
    transformed_data = transform_data()
    
    print(f"âœ… TransformaciÃ³n completada. Registros transformados: {len(transformed_data) if transformed_data is not None else 0}")
    
    return {"status": "success", "transformed_records": len(transformed_data) if transformed_data is not None else 0}

def load_task(**context):
    """
    Task para cargar datos
    """
    print("ğŸ“¥ Iniciando carga de datos...")
    
    # Obtener resultado del task anterior
    transform_result = context['task_instance'].xcom_pull(task_ids='transform_data')
    print(f"ğŸ“Š Resultado de transformaciÃ³n: {transform_result}")
    
    # Llamar a tu funciÃ³n de carga
    result = load_data()
    
    print("âœ… Carga completada exitosamente")
    
    return {"status": "success", "load_result": result}

def check_data_quality(**context):
    """
    Task para verificar calidad de datos
    """
    print("ğŸ” Verificando calidad de datos...")
    
    # Obtener resultados de tasks anteriores
    extract_result = context['task_instance'].xcom_pull(task_ids='extract_data')
    transform_result = context['task_instance'].xcom_pull(task_ids='transform_data')
    load_result = context['task_instance'].xcom_pull(task_ids='load_data')
    
    print(f"ğŸ“Š Resumen del pipeline:")
    print(f"   - ExtracciÃ³n: {extract_result}")
    print(f"   - TransformaciÃ³n: {transform_result}")
    print(f"   - Carga: {load_result}")
    
    # AquÃ­ puedes aÃ±adir validaciones adicionales
    if (extract_result and extract_result.get('status') == 'success' and 
        transform_result and transform_result.get('status') == 'success' and
        load_result and load_result.get('status') == 'success'):
        print("âœ… Pipeline completado exitosamente - Calidad de datos OK")
        return {"status": "success", "quality_check": "passed"}
    else:
        raise ValueError("âŒ Pipeline fallÃ³ - Verificar logs")

# Crear tasks
extract_task_op = PythonOperator(
    task_id='extract_data',
    python_callable=extract_task,
    dag=dag,
)

transform_task_op = PythonOperator(
    task_id='transform_data',
    python_callable=transform_task,
    dag=dag,
)

load_task_op = PythonOperator(
    task_id='load_data',
    python_callable=load_task,
    dag=dag,
)

quality_check_task_op = PythonOperator(
    task_id='data_quality_check',
    python_callable=check_data_quality,
    dag=dag,
)

# Task de inicio
start_task = BashOperator(
    task_id='start_pipeline',
    bash_command='echo "ğŸš€ Iniciando pipeline ETL..."',
    dag=dag,
)

# Task de fin
end_task = BashOperator(
    task_id='end_pipeline',
    bash_command='echo "ğŸ‰ Pipeline ETL completado exitosamente!"',
    dag=dag,
)

# Definir dependencias (orden de ejecuciÃ³n)
start_task >> extract_task_op >> transform_task_op >> load_task_op >> quality_check_task_op >> end_task